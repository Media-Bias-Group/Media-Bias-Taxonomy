{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper for Semantic Scholar (SES) and DBLP\n",
    "This a scraping tool, that uses the REST APIs of Semantic Scholar and DBLP to retrieve literature from both platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Use the following segment for the setup up your literature search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Parameters\n",
    "You can set up your search parameters in the following code segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What platforms do you want to search?\n",
    "# Semantic Scholar\n",
    "search_semantic_scholar = True\n",
    "# DBLP\n",
    "search_dblp = True\n",
    "\n",
    "# search literature published between year_range_start and year_range_end\n",
    "# set each to False if you want to search all literature up to a year or from a specific year onwards\n",
    "# if you use DBLP, specify both\n",
    "year_range_start = 2019\n",
    "year_range_end = 2022\n",
    "\n",
    "# enter your Semantic Scholar key here:\n",
    "SSkey = \"INSERT YOUR KEY HERE\"\n",
    "\n",
    "# how many results per keyword should be scraped from Semantic Scholar?\n",
    "max_ses = 300\n",
    "\n",
    "# select returned fields or set to FALSE\n",
    "# set to False if only default parameters are required\n",
    "return_fieldes =[\"paperId\",\n",
    "                 \"externalIds\", \n",
    "                 \"url\",\n",
    "                 \"title\",\n",
    "                 \"abstract\",\n",
    "                 \"venue\",\n",
    "                 \"publicationVenue\",\n",
    "                 \"year\",\n",
    "                 \"referenceCount\",\n",
    "                 \"citationCount\",\n",
    "                 \"influentialCitationCount\",\n",
    "                 \"isOpenAccess\",\n",
    "                 \"openAccessPdf\",\n",
    "                 \"fieldsOfStudy\",\n",
    "                 \"publicationTypes\",\n",
    "                 \"publicationDate\",\n",
    "                 \"journal\",\n",
    "                 \"citationStyles\",\n",
    "                 \"authors\"]\n",
    "\n",
    "# select the type of publication or False to not filter by Publication type\n",
    "publication_type  = False\n",
    "# select from Review, JournalArticle, CaseReport, ClinicalTrial, Dataset, Editorial, LettersAndComments, MetaAnalysis, News, Study, Book, BookSection\n",
    "\n",
    "\n",
    "# select the fields of study to filter for or set to False\n",
    "fields_of_Study = [\"Computer Science\",\n",
    "                   \"Medicine\",\n",
    "                   \"Chemistry\",\n",
    "                   \"Biology\",\n",
    "                   \"Materials Science\",\n",
    "                   \"Physics\",\n",
    "                   \"Geology\",\n",
    "                   \"Psychology\",\n",
    "                   \"Art\",\n",
    "                   \"History\",\n",
    "                   \"Geography\",\n",
    "                   \"Sociology\",\n",
    "                   \"Business\",\n",
    "                   \"Political Science\",\n",
    "                   \"Economics\",\n",
    "                   \"Philosophy\",\n",
    "                   \"Mathematics\",\n",
    "                   \"Engineering\",\n",
    "                   \"Environmental Science\",\n",
    "                   \"Agricultural and Food Sciences\",\n",
    "                   \"Education\",\n",
    "                   \"Law\",\n",
    "                   \"Linguistics\"]\n",
    "    \n",
    "    \n",
    "# filenames for export\n",
    "ses_filename = \"semScho_publications_export.csv\"\n",
    "dblp_filename = \"dblp_publications_export.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preparing Keywords\n",
    "Provide a keyword file  named \"keywords_ses.txt\" for SES and \"keywords_ses.txt\" for DBLP, in which each line contains a search query. The scraper will transform the query to the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SES keywords\n",
    "keywords_ses = []\n",
    "with open(\"keywords_ses.txt\", \"r\") as file:\n",
    "    \n",
    "    for line in file.readlines():\n",
    "        keywords_ses.append(line.replace(\"\\n\",\"\").lower())\n",
    "\n",
    "\n",
    "# Load DBLP keywords\n",
    "keywords_dblp = []\n",
    "# with open(\"keywords_dblp.txt\", \"r\") as file:\n",
    "with open(\"keywords_ses.txt\", \"r\") as file:\n",
    "    \n",
    "    for line in file.readlines():\n",
    "        keywords_dblp.append(line.replace(\"\\n\",\"\").lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation of SES parameters\n",
    "\n",
    "if fields_of_Study:\n",
    "    fields_of_Study = \",\".join([a.replace(\" \", \"+\") for a in fields_of_Study])\n",
    "\n",
    "if year_range_start:\n",
    "    if year_range_end:\n",
    "        if year_range_end == year_range_start:\n",
    "            year = str(year_range_start)\n",
    "            year_range = [year_range_start]\n",
    "        else:\n",
    "            year = str(year_range_start)+\"-\"+str(year_range_end)\n",
    "            year_range = [y for y in range (year_range_start, year_range_end+1)]\n",
    "    else:\n",
    "        year = str(year_range_start)+\"-\"\n",
    "else:\n",
    "    if year_range_end:\n",
    "        if year_range_end == year_range_start:\n",
    "            year = str(year_range_start)\n",
    "        else:\n",
    "            year = str(year_range_start)+\"-\"+str(year_range_end)\n",
    "    else:\n",
    "        year = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import of modules and definition of methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate SES url based on keyword input\n",
    "def create_url_from_keyw(keyword):\n",
    "    pre = \"https://api.semanticscholar.org/graph/v1/paper/search?\"\n",
    "    q = \"&query=\"+keyword.replace(\" \",\"+\")\n",
    "    post = \"&offset=0&limit=100&x-api-key=\"+SSkey+\"&sort=relevance\"\n",
    "    \n",
    "    if year:\n",
    "        y = \"&year=\"+year\n",
    "    else:\n",
    "        y = \"\"\n",
    "        \n",
    "    if return_fieldes:\n",
    "        ret = \"&fields=\"+\",\".join(return_fieldes)\n",
    "    else:\n",
    "        ret = \"\"\n",
    "    \n",
    "    if fields_of_Study:\n",
    "        fos = \"&fieldsOfStudy=\"+fields_of_Study\n",
    "    else: \n",
    "        fos = \"\" \n",
    "    \n",
    "    if publication_type:\n",
    "        p_type = \"&publicationTypes=\"+publication_type\n",
    "    else :\n",
    "        p_type = \"\"\n",
    "        \n",
    "    return pre+q+y+post+ret+fos+p_type\n",
    "\n",
    "# generate dblp url based on keyword input\n",
    "def create_url_from_keyw_dblp(keyword):\n",
    "    pre = \"https://dblp.org/search/publ/api?\"\n",
    "    alt_pre = \"https://dblp.uni-trier.de.org/search/publ/api?\"\n",
    "    alt_pre2 = \"https://dblp2.uni-trier.de/search/publ/api?\"\n",
    "    q = \"q=\"+keyword.replace(\" \",\"+\")\n",
    "    cap = \"&h=1000&format=json\"\n",
    "    return pre+q+cap\n",
    "\n",
    "# SES scrape method \n",
    "def create_database_ses(urllist, keyw, results_per_keyw=100):\n",
    "    exp = {}\n",
    "    debug = []\n",
    "    for i, url in enumerate(urllist):\n",
    "        exp[keyw[i]] = []\n",
    "        res = []\n",
    "        print(i+1,\"/\",len(keyw),\":\", keyw[i])\n",
    "        \n",
    "        max_offset = results_per_keyw-100\n",
    "        curr_offset = 0\n",
    "        \n",
    "        # first 100 documents\n",
    "        text = requests.get(url).text\n",
    "        res_json = json.loads(text)\n",
    "        if \"data\" in res_json.keys():\n",
    "            res.extend(json.loads(text)[\"data\"])\n",
    "        else:\n",
    "            debug.append(res_json)\n",
    "        exp[keyw[i]].extend(res)\n",
    "        print(str(len(res))+\" entries retrieved\", end=\"\")\n",
    "        time.sleep(1)\n",
    "        print(\"\", end=\"\\r\")\n",
    "        \n",
    "        # iterations until max: \n",
    "        while curr_offset < max_offset:\n",
    "            url = url.replace(\"offset=\"+str(curr_offset),\"offset=\"+str(curr_offset+100))\n",
    "            curr_offset += 100\n",
    "            text = requests.get(url).text\n",
    "            res_json = json.loads(text)\n",
    "            if \"data\" in res_json.keys():\n",
    "                res.extend(json.loads(text)[\"data\"])\n",
    "            else:\n",
    "                print(\"there was a problem retrieving or saving publications.\")\n",
    "                debug.append(res_json)\n",
    "            \n",
    "            print(str(len(res))+\" publications retrieved\", end=\"\")\n",
    "            print(\"\", end=\"\\r\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "            \n",
    "            exp[keyw[i]].extend(res)\n",
    "            \n",
    "        clear_output()\n",
    "\n",
    "    return exp, debug\n",
    "\n",
    "\n",
    "# for matching papers to search terms by ID\n",
    "def check_id(paper_id, data_dic, criter):\n",
    "    ret = []\n",
    "    for search_term in data_dic.keys():\n",
    "        for paper in data_dic[search_term]:\n",
    "            if paper[criter] == paper_id:\n",
    "                if search_term not in ret:\n",
    "                    ret.append(search_term)\n",
    "    return ret\n",
    "\n",
    "# creates string out of dict of authors\n",
    "def get_authors(dic):\n",
    "    ret = \"\"\n",
    "    for i, p in enumerate(dic):\n",
    "        if i==0:\n",
    "            ret = p[\"name\"]\n",
    "        else: \n",
    "            ret = ret+\", \"+p[\"name\"]\n",
    "    return ret\n",
    "\n",
    "# creates string for dblp return format\n",
    "def get_authors_dblp(dic):\n",
    "    ret = \"\"\n",
    "    if type(dic)==float:\n",
    "        return \"\"\n",
    "    if type(dic[\"author\"]) == dict:\n",
    "        return dic[\"author\"][\"text\"]\n",
    "    for i, p in enumerate(dic[\"author\"]):\n",
    "        if i==0:\n",
    "            ret = p[\"text\"]\n",
    "        else: \n",
    "            ret = ret+\", \"+p[\"text\"]\n",
    "    return ret\n",
    "\n",
    "# scrape dblp for given years\n",
    "# provide years as list of ints\n",
    "# keyw is list of keywords that was used to generate urls and is only used for output and structuring\n",
    "def get_dblp_data_years(urls, keyw, years):\n",
    "    exp = {}\n",
    "    print(\"retrieving literature from year range:\", years)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for i, url in enumerate(urls):\n",
    "        print(i+1,\"/\",len(keyw),\":\", keyw[i])\n",
    "        \n",
    "        text = requests.get(url).text\n",
    "        res_json = json.loads(text)\n",
    "        \n",
    "        # retrieve results\n",
    "        print(int(res_json[\"result\"][\"hits\"][\"@total\"]),\"entries found in total for \\\"\",keyw[i],\"\\\"\")\n",
    "        if (int(res_json[\"result\"][\"hits\"][\"@total\"]) > 0):\n",
    "            finds = (res_json[\"result\"][\"hits\"][\"hit\"])\n",
    "            \n",
    "        # filter by year range\n",
    "        clean_results = [a[\"info\"] for a in finds if int(a[\"info\"][\"year\"]) in years]\n",
    "        exp[keyw[i].replace(\" \", \"_\")] = clean_results\n",
    "        print(\"entries in year range >>\",len(clean_results))\n",
    "        time.sleep(3)\n",
    "        clear_output()\n",
    "        \n",
    "        \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates lists uf URLS for REST API access\n",
    "if search_semantic_scholar:\n",
    "    urls = [create_url_from_keyw(key) for key in keywords_ses]\n",
    "if search_dblp:\n",
    "    urls_dblp = [create_url_from_keyw_dblp(key) for key in keywords_dblp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Literature\n",
    "The following section performs the actual search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get semantic scholar results:\n",
    "\n",
    "if search_semantic_scholar:\n",
    "    raw_data, raw_bugreport = create_database_ses(urls, keywords_ses, max_ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get dblp results:\n",
    "\n",
    "if search_dblp:\n",
    "    raw_data_dblp = get_dblp_data_years(urls_dblp, keywords_dblp, year_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean results and prepare output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SES\n",
    "if search_semantic_scholar:\n",
    "    dFrames_ses = {}   \n",
    "    for d in raw_data.keys(): \n",
    "        dFrames_ses[d] = pd.DataFrame(raw_data[d])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBLP\n",
    "if search_dblp:\n",
    "    dFrames_dblp = {}   \n",
    "    for d in raw_data_dblp.keys():\n",
    "        dFrames_dblp[d] = pd.DataFrame(raw_data_dblp[d])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates from scrapes for different keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Scholar\n",
    "if search_semantic_scholar:\n",
    "    # join results from all searches\n",
    "    df_ses = pd.concat(dFrames_ses.values())\n",
    "\n",
    "    # filter duplicates\n",
    "    df_no_dupes_ses = df_ses.drop_duplicates(subset=\"paperId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBLP\n",
    "if search_dblp:\n",
    "    # join results from all searches\n",
    "    df_dblp = pd.concat(dFrames_dblp.values())\n",
    "\n",
    "    # filter duplicates\n",
    "    df_no_dupes_dblp = df_dblp.drop_duplicates(subset=\"url\")\n",
    "\n",
    "    # filter semantic scholar by date\n",
    "    data_export_dblp = df_no_dupes_dblp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert authors to exportable Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SES\n",
    "if search_semantic_scholar:\n",
    "    data_export_cs = df_no_dupes_ses.astype({'year': 'int'})\n",
    "\n",
    "    # add tags for search queries\n",
    "    data_export_cs[\"searchQuery\"] = data_export_cs.apply(lambda row: check_id(row[\"paperId\"], raw_data, \"paperId\") , axis=1)\n",
    "\n",
    "    # add clear authors column\n",
    "    data_export_cs[\"authors_clear\"] = data_export_cs.apply(lambda row: get_authors(row[\"authors\"]) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBLP\n",
    "if search_dblp:\n",
    "    # add tags for search queries\n",
    "    data_export_dblp[\"searchQuery\"] = data_export_dblp.apply(lambda row: check_id(row[\"url\"], raw_data_dblp, \"url\") , axis=1)\n",
    "\n",
    "    # add clear authors column\n",
    "    data_export_dblp[\"authors_clear\"] = data_export_dblp.apply(lambda row: get_authors_dblp(row[\"authors\"]) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "if search_semantic_scholar:\n",
    "    data_export_cs.to_csv(ses_filename)\n",
    "\n",
    "if search_dblp:\n",
    "    data_export_dblp.to_csv(dblp_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
